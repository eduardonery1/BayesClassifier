{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def fit(self, X, y):\n",
    "        num_docs, vocab_size = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        num_classes = len(unique_classes)\n",
    "\n",
    "        # Calculate class probabilities\n",
    "        self.class_probs = np.zeros(num_classes)\n",
    "        for i, c in enumerate(unique_classes):\n",
    "            self.class_probs[i] = np.sum(y == c) / num_docs\n",
    "\n",
    "        # Calculate word probabilities with Laplace smoothing\n",
    "        self.word_probs = np.zeros((num_classes, vocab_size))\n",
    "        for i, c in enumerate(unique_classes):\n",
    "            class_docs = X[y == c]\n",
    "            total_words_in_class = np.sum(class_docs)\n",
    "            self.word_probs[i] = (np.sum(class_docs, axis=0) + self.alpha) / (total_words_in_class + self.alpha * vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(self, X):\n",
    "        num_docs, _ = X.shape\n",
    "        num_classes, vocab_size = self.word_probs.shape\n",
    "\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "\n",
    "        # Calculate log likelihoods for each class\n",
    "        log_likelihoods = np.zeros((num_docs, num_classes))\n",
    "        for i in range(num_docs):\n",
    "            doc = X[i].toarray().flatten()  # Convert the sparse matrix to a dense array\n",
    "            log_likelihoods[i] = np.sum(log_word_probs * doc, axis=1) + log_class_probs\n",
    "\n",
    "        # Convert log likelihoods to probabilities using softmax\n",
    "        exp_log_likelihoods = np.exp(log_likelihoods - np.max(log_likelihoods, axis=1, keepdims=True))\n",
    "        probabilities = exp_log_likelihoods / np.sum(exp_log_likelihoods, axis=1, keepdims=True)\n",
    "\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        num_docs, _ = X.shape\n",
    "        num_classes, vocab_size = self.word_probs.shape\n",
    "\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "\n",
    "        # Calculate log likelihoods for each class\n",
    "        log_likelihoods = np.zeros((num_docs, num_classes))\n",
    "        for i in range(num_docs):\n",
    "            doc = X[i].toarray().flatten()  # Convert the sparse matrix to a dense array\n",
    "            log_likelihoods[i] = np.sum(log_word_probs * doc, axis=1) + log_class_probs\n",
    "\n",
    "        # Predict the class with the highest log likelihood\n",
    "        predictions = np.argmax(log_likelihoods, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/sda/Projects/BayesClassifier/classifier.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[1;32m      <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#Setting up stop words and punctuation while loading the Spacy model.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m stop_words \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mDefaults\u001b[39m.\u001b[39mstop_words\n\u001b[1;32m     <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m punctuations \u001b[39m=\u001b[39m string\u001b[39m.\u001b[39mpunctuation\n",
      "File \u001b[0;32m~/miniconda3/envs/NYU-DL/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name,\n\u001b[1;32m     53\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m     54\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m     55\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[1;32m     56\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m     57\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     58\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/NYU-DL/lib/python3.10/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Setting up stop words and punctuation while loading the Spacy model.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "punctuations = string.punctuation\n",
    "\n",
    "def spacy_tokenizer(sentence, count_words=False):\n",
    "    if type(sentence) == float:\n",
    "        sentence = sentence\n",
    "    else:\n",
    "        doc = nlp(sentence)\n",
    "        mytokens = [word.lemma_.lower().strip() for word in doc]\n",
    "        mytokens = [word for word in mytokens if word not in stop_words and word not in punctuations]\n",
    "        sentence = \" \".join(mytokens)\n",
    "\n",
    "    if count_words:\n",
    "        word_count = len(mytokens)\n",
    "        return sentence, word_count\n",
    "    else:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/sda/Projects/BayesClassifier/classifier.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mllm-detect-ai-generated-text/train_essays.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Tokenize and preprocess the text data\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/sda/Projects/BayesClassifier/classifier.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data[[\u001b[39m'\u001b[39m\u001b[39mtokenized_Response\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: pd\u001b[39m.\u001b[39mSeries(spacy_tokenizer(x, count_words\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"llm-detect-ai-generated-text/train_essays.csv\")\n",
    "\n",
    "# Tokenize and preprocess the text data\n",
    "data[['tokenized_Response', 'count']] = data['text'].apply(lambda x: pd.Series(spacy_tokenizer(x, count_words=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary and reverse the index.\n",
    "vectorizer = CountVectorizer()\n",
    "X_vec = vectorizer.fit_transform(data['tokenized_Response'])\n",
    "vocab = vectorizer.get_feature_names_out() \n",
    "reverse_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Determine the likelihood of happening.\n",
    "total_documents = len(data)\n",
    "word_occurrences = np.array(X_vec.sum(axis=0)).flatten()\n",
    "\n",
    "occurrence_probs = word_occurrences / total_documents\n",
    "\n",
    "# Determine the conditional probability based on the classification (human or LLM).\n",
    "llm_indices = data[data['generated'] == 1].index\n",
    "human_indices = data[data['generated'] == 0].index\n",
    "llm_documents = len(llm_indices)\n",
    "human_documents = len(human_indices)\n",
    "\n",
    "llm_word_occurrences = np.array(X_vec[llm_indices].sum(axis=0)).flatten()\n",
    "human_word_occurrences = np.array(X_vec[human_indices].sum(axis=0)).flatten()\n",
    "\n",
    "llm_probs = llm_word_occurrences / llm_documents\n",
    "human_probs = human_word_occurrences / human_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier Implementation\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "        self.class_probs = None\n",
    "        self.word_probs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_docs, vocab_size = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "        num_classes = len(unique_classes)\n",
    "\n",
    "        # Calculate class probabilities\n",
    "        self.class_probs = np.zeros(num_classes)\n",
    "        for i, c in enumerate(unique_classes):\n",
    "            self.class_probs[i] = np.sum(y == c) / num_docs\n",
    "\n",
    "        # Calculate word probabilities with Laplace smoothing\n",
    "        self.word_probs = np.zeros((num_classes, vocab_size))\n",
    "        for i, c in enumerate(unique_classes):\n",
    "            class_docs = X[y == c]\n",
    "            total_words_in_class = np.sum(class_docs)\n",
    "            self.word_probs[i] = (np.sum(class_docs, axis=0) + self.alpha) / (total_words_in_class + self.alpha * vocab_size)\n",
    "\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        num_docs, _ = X.shape\n",
    "        num_classes, vocab_size = self.word_probs.shape\n",
    "\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "\n",
    "        # Calculate log likelihoods for each class\n",
    "        log_likelihoods = np.zeros((num_docs, num_classes))\n",
    "        for i in range(num_docs):\n",
    "            doc = X[i].toarray().flatten()  # Convert the sparse matrix to a dense array\n",
    "            log_likelihoods[i] = np.sum(log_word_probs * doc, axis=1) + log_class_probs\n",
    "\n",
    "        # Convert log likelihoods to probabilities using softmax\n",
    "        exp_log_likelihoods = np.exp(log_likelihoods - np.max(log_likelihoods, axis=1, keepdims=True))\n",
    "        probabilities = exp_log_likelihoods / np.sum(exp_log_likelihoods, axis=1, keepdims=True)\n",
    "\n",
    "        return probabilities        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        num_docs, _ = X.shape\n",
    "        num_classes, vocab_size = self.word_probs.shape\n",
    "\n",
    "        # Use log probabilities to avoid underflow\n",
    "        log_class_probs = np.log(self.class_probs)\n",
    "        log_word_probs = np.log(self.word_probs)\n",
    "\n",
    "        # Calculate log likelihoods for each class\n",
    "        log_likelihoods = np.zeros((num_docs, num_classes))\n",
    "        for i in range(num_docs):\n",
    "            doc = X[i].toarray().flatten()  # Convert the sparse matrix to a dense array\n",
    "            log_likelihoods[i] = np.sum(log_word_probs * doc, axis=1) + log_class_probs\n",
    "\n",
    "        # Predict the class with the highest log likelihood\n",
    "        predictions = np.argmax(log_likelihoods, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_value = 1\n",
    "custom_nb_classifier = NaiveBayesClassifier(alpha=alpha_value)\n",
    "custom_nb_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_custom = custom_nb_classifier.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification Report on Dev Set:\")\n",
    "print(classification_report(y_pred_custom, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the effect of Smoothing\n",
    "alpha_values = [0.1, 0.5, 1, 2, 5]\n",
    "for alpha in alpha_values:\n",
    "    custom_nb_classifier = NaiveBayesClassifier(alpha=alpha)\n",
    "    custom_nb_classifier.fit(x_train, y_train)\n",
    "    y_pred_custom = custom_nb_classifier.predict(x_dev)\n",
    "    accuracy = np.mean(y_pred_custom == y_dev)\n",
    "    print(f\"Accuracy with Smoothing (alpha={alpha}): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive Top 10 words that predict each class\n",
    "top_words_llm = [vocab[i] for i in np.argsort(llm_probs)[-10:]]\n",
    "top_words_human = [vocab[i] for i in np.argsort(human_probs)[-10:]]\n",
    "\n",
    "print(\"Top 10 words predicting LLM:\")\n",
    "print(top_words_llm)\n",
    "print(\"\\nTop 10 words predicting Human:\")\n",
    "print(top_words_human)\n",
    "\n",
    "# Using the test dataset\n",
    "test_df = pd.read_csv('llm-detect-ai-generated-text/test_essays.csv')\n",
    "test_df['tokenized_Response'] = test_df['text'].apply(spacy_tokenizer)\n",
    "test_vec = vectorizer.transform(test_df['tokenized_Response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('llm-detect-ai-generated-text/sample_submission.csv')\n",
    "\n",
    "# custom_nb_classifier is the trained Naive Bayes classifier\n",
    "for i in range(len(submit)):\n",
    "    output_arr = custom_nb_classifier.predict_proba(test_vec[i])\n",
    "    submit.iloc[i, 1] = output_arr[0][0]\n",
    "\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NYU-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
